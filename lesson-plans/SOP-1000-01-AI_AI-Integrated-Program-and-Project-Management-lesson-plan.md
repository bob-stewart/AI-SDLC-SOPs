# Lesson Plan Draft: AI-Integrated Program and Project Management (AI-SDLC)

**SOP Reference:** SOP-1000-01-AI_AI-Integrated-Program-and-Project-Management

## Objective

This SOP defines the *Program and Project Management* framework for AI-Integrated Systems Development Life Cycle (AI-SDLC) at Horizon. It includes processes for engaging the AI Institutional Review Board (AI-IRB) when a project involves regulated AI components or high-risk AI features. The objective is to ensure:

1. Compliance with relevant AI governance, ethical guidelines, and regulatory standards (including AI-IRB).  
2. Efficiency in delivering AI-enabled products or services, balancing scope, cost, and schedule constraints.  
3. Quality in design, documentation, and rollout of new or enhanced functionalities that incorporate AI or ML (machine learning) modules.  
4. Traceability from initial business need through final deployment and post-implementation review.

## Audience & Applicability

Covers all AI-related program and project management activities under the AI-SDLC umbrella. Includes focus areas such as Initiation, Planning, Execution & Monitoring, and Close-out. Applies to all Horizon teams: Product Development, AI-PMO, Engineering (Development, QA, Ops), Technical Support, and any external or contract resources.

## Key Definitions

- **AI-IRB**: *AI Institutional Review Board*, a governance body ensuring ethical and compliant oversight of AI modules, focusing on risk, bias, and regulatory compliance.
- **AI-SDLC**: *AI-Integrated Systems Development Life Cycle*, a methodology that extends classical SDLC with specialized AI design, model governance, ethics, and monitoring controls.
- **Program**: A collection of AI-related projects that share dependencies, resources, or strategic goals.
- **Project**: A temporary endeavor to create or enhance AI-driven products/services with a defined scope, time, and cost constraints.
- **Gate (Business Gate)**: A defined checkpoint in the AI-SDLC that requires specific deliverables to be approved before proceeding. Example: Gate 10 (Requirements Lock-Down), Gate 6 (Project Lock-Down), Gate 0 (General Availability).
- **Deliverable**: Any project output requiring formal review/approval (e.g., AI risk assessment, system design, code modules, test results, AI model performance metrics).
- **Performing Organization**: Typically the Engineering Department (Development, QA, Ops, AI specialists).
- **Contracting Organization**: Typically the Product/Technical Support groups or external clients who define requirements and accept deliverables.

## Key Roles

- **Senior Management**: Establishes overall AI strategy and R&D budget; Commissions new AI projects; appoints AI Project Sponsors; Provides final decisions for resource trade-offs.
- **AI-IRB**: Evaluates high-risk AI designs/changes for ethical, legal, and regulatory compliance; Issues formal approval or requests rework for AI modules prior to release.
- **Project Sponsor**: Champions the AI-related project from concept to closure; Escalates issues to Senior Management; Final authority on scope changes, cost, schedule trade-offs for the project.
- **AI-PMO**: Oversees the consolidated portfolio of AI-centric projects; Tracks cross-project dependencies, resource conflicts; Issues periodic status to Senior Management; ensures common standards, policies, and reporting.
- **Product Manager**: Defines the business requirements for AI features; Aligns AI solutions to strategic goals; consults with AI-IRB if risk classification is uncertain; Coordinates user acceptance criteria with QA & end-users.
- **Program Manager**: Manages day-to-day tasks across multiple AI projects under a single program; Ensures gating deliverables are completed; organizes reviews with AI-IRB, QA, and other stakeholders; Coordinates risk management activities.
- **Project Manager**: Single project focus: planning, scheduling, budget management; Coordinates tasks among development, QA, AI-IRB, and operations; Maintains project documentation, issue logs, change requests.
- **Development**: Implements AI system features and code changes; Provides estimates, tracks actuals vs. estimates; Submits technical design docs and code to QA & AI-IRB as necessary.
- **Quality Assurance (QA)**: Defines test strategies for AI systems (functionality, data bias, model performance, ethics compliance); Approves system readiness at each gate; logs defects; Coordinates with AI-IRB on measuring AI compliance.
- **Operations**: Provides infrastructure for AI model deployment, environment provisioning, version control, site monitoring; Executes deployment steps in production.
- **Technical Support**: Acts as service organization and possibly training group; Provides first-level support for AI product usage after deployment.

